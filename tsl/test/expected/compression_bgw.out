-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
\c :TEST_DBNAME :ROLE_SUPERUSER
CREATE ROLE NOLOGIN_ROLE WITH nologin noinherit;
GRANT NOLOGIN_ROLE TO :ROLE_DEFAULT_PERM_USER WITH ADMIN OPTION;
\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
CREATE TABLE conditions (
      time        TIMESTAMPTZ       NOT NULL,
      location    TEXT              NOT NULL,
      location2    char(10)              NOT NULL,
      temperature DOUBLE PRECISION  NULL,
      humidity    DOUBLE PRECISION  NULL
    );
select create_hypertable( 'conditions', 'time', chunk_time_interval=> '31days'::interval);
    create_hypertable    
-------------------------
 (1,public,conditions,t)
(1 row)

--TEST 1--
--cannot set policy without enabling compression --
\set ON_ERROR_STOP 0
select add_compression_policy('conditions', '60d'::interval);
ERROR:  compression not enabled on hypertable "conditions"
\set ON_ERROR_STOP 1
-- TEST2 --
--add a policy to compress chunks --
alter table conditions set (timescaledb.compress, timescaledb.compress_segmentby = 'location', timescaledb.compress_orderby = 'time');
insert into conditions
select generate_series('2018-12-01 00:00'::timestamp, '2018-12-31 00:00'::timestamp, '1 day'), 'POR', 'klick', 55, 75;
select add_compression_policy('conditions', '60d'::interval) AS compressjob_id
\gset
select * from _timescaledb_config.bgw_job where id = :compressjob_id;
  id  |     application_name      | schedule_interval  | max_runtime | max_retries | retry_period |      proc_schema      |     proc_name      |       owner       | scheduled | hypertable_id |                       config                        
------+---------------------------+--------------------+-------------+-------------+--------------+-----------------------+--------------------+-------------------+-----------+---------------+-----------------------------------------------------
 1000 | Compression Policy [1000] | @ 15 days 12 hours | @ 0         |          -1 | @ 1 hour     | _timescaledb_internal | policy_compression | default_perm_user | t         |             1 | {"hypertable_id": 1, "compress_after": "@ 60 days"}
(1 row)

select * from alter_job(:compressjob_id, schedule_interval=>'1s');
 job_id | schedule_interval | max_runtime | max_retries | retry_period | scheduled |                       config                        | next_start 
--------+-------------------+-------------+-------------+--------------+-----------+-----------------------------------------------------+------------
   1000 | @ 1 sec           | @ 0         |          -1 | @ 1 hour     | t         | {"hypertable_id": 1, "compress_after": "@ 60 days"} | -infinity
(1 row)

select * from _timescaledb_config.bgw_job where id >= 1000 ORDER BY id;
  id  |     application_name      | schedule_interval | max_runtime | max_retries | retry_period |      proc_schema      |     proc_name      |       owner       | scheduled | hypertable_id |                       config                        
------+---------------------------+-------------------+-------------+-------------+--------------+-----------------------+--------------------+-------------------+-----------+---------------+-----------------------------------------------------
 1000 | Compression Policy [1000] | @ 1 sec           | @ 0         |          -1 | @ 1 hour     | _timescaledb_internal | policy_compression | default_perm_user | t         |             1 | {"hypertable_id": 1, "compress_after": "@ 60 days"}
(1 row)

insert into conditions
select now()::timestamp, 'TOK', 'sony', 55, 75;
-- TEST3 --
--only the old chunks will get compressed when policy is executed--
CALL run_job(:compressjob_id);
select chunk_name, pg_size_pretty(before_compression_total_bytes) before_total,
pg_size_pretty( after_compression_total_bytes)  after_total
from chunk_compression_stats('conditions') where compression_status like 'Compressed' order by chunk_name;
    chunk_name    | before_total | after_total 
------------------+--------------+-------------
 _hyper_1_1_chunk | 32 kB        | 32 kB
(1 row)

SELECT * FROM _timescaledb_catalog.chunk ORDER BY id;
 id | hypertable_id |      schema_name      |        table_name        | compressed_chunk_id | dropped | status 
----+---------------+-----------------------+--------------------------+---------------------+---------+--------
  1 |             1 | _timescaledb_internal | _hyper_1_1_chunk         |                   4 | f       |      1
  2 |             1 | _timescaledb_internal | _hyper_1_2_chunk         |                     | f       |      0
  3 |             1 | _timescaledb_internal | _hyper_1_3_chunk         |                     | f       |      0
  4 |             2 | _timescaledb_internal | compress_hyper_2_4_chunk |                     | f       |      0
(4 rows)

-- TEST 4 --
--cannot set another policy
\set ON_ERROR_STOP 0
select add_compression_policy('conditions', '60d'::interval, if_not_exists=>true);
NOTICE:  compression policy already exists for hypertable "conditions", skipping
 add_compression_policy 
------------------------
                     -1
(1 row)

select add_compression_policy('conditions', '60d'::interval);
ERROR:  compression policy already exists for hypertable "conditions"
select add_compression_policy('conditions', '30d'::interval, if_not_exists=>true);
WARNING:  compression policy already exists for hypertable "conditions"
 add_compression_policy 
------------------------
                     -1
(1 row)

\set ON_ERROR_STOP 1
--TEST 5 --
-- drop the policy --
select remove_compression_policy('conditions');
 remove_compression_policy 
---------------------------
 t
(1 row)

select count(*) from _timescaledb_config.bgw_job WHERE id>=1000;
 count 
-------
     0
(1 row)

--TEST 6 --
-- try to execute the policy after it has been dropped --
\set ON_ERROR_STOP 0
CALL run_job(:compressjob_id);
ERROR:  job 1000 not found
\set ON_ERROR_STOP 1
-- We're done with the table, so drop it.
DROP TABLE IF EXISTS conditions CASCADE;
NOTICE:  drop cascades to table _timescaledb_internal.compress_hyper_2_4_chunk
--TEST 7
--compression policy for integer based partition hypertable
CREATE TABLE test_table_int(time bigint, val int);
SELECT create_hypertable('test_table_int', 'time', chunk_time_interval => 1);
NOTICE:  adding not-null constraint to column "time"
      create_hypertable      
-----------------------------
 (3,public,test_table_int,t)
(1 row)

create or replace function dummy_now() returns BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 5::BIGINT';
select set_integer_now_func('test_table_int', 'dummy_now');
 set_integer_now_func 
----------------------
 
(1 row)

insert into test_table_int select generate_series(1,5), 10;
alter table test_table_int set (timescaledb.compress);
select add_compression_policy('test_table_int', 2::int) AS compressjob_id
\gset
select * from _timescaledb_config.bgw_job where id=:compressjob_id;
  id  |     application_name      | schedule_interval | max_runtime | max_retries | retry_period |      proc_schema      |     proc_name      |       owner       | scheduled | hypertable_id |                  config                   
------+---------------------------+-------------------+-------------+-------------+--------------+-----------------------+--------------------+-------------------+-----------+---------------+-------------------------------------------
 1001 | Compression Policy [1001] | @ 1 day           | @ 0         |          -1 | @ 1 hour     | _timescaledb_internal | policy_compression | default_perm_user | t         |             3 | {"hypertable_id": 3, "compress_after": 2}
(1 row)

\gset
CALL run_job(:compressjob_id);
CALL run_job(:compressjob_id);
select chunk_name, before_compression_total_bytes, after_compression_total_bytes
from chunk_compression_stats('test_table_int') where compression_status like 'Compressed' order by chunk_name;
    chunk_name    | before_compression_total_bytes | after_compression_total_bytes 
------------------+--------------------------------+-------------------------------
 _hyper_3_5_chunk |                          24576 |                         16384
 _hyper_3_6_chunk |                          24576 |                         16384
(2 rows)

--TEST 8
--hypertable owner lacks permission to start background worker
SET ROLE NOLOGIN_ROLE;
CREATE TABLE test_table_nologin(time bigint, val int);
SELECT create_hypertable('test_table_nologin', 'time', chunk_time_interval => 1);
NOTICE:  adding not-null constraint to column "time"
        create_hypertable        
---------------------------------
 (5,public,test_table_nologin,t)
(1 row)

SELECT set_integer_now_func('test_table_nologin', 'dummy_now');
 set_integer_now_func 
----------------------
 
(1 row)

ALTER TABLE test_table_nologin set (timescaledb.compress);
\set ON_ERROR_STOP 0
SELECT add_compression_policy('test_table_nologin', 2::int);
ERROR:  permission denied to start background process as role "nologin_role"
\set ON_ERROR_STOP 1
RESET ROLE;
REVOKE NOLOGIN_ROLE FROM :ROLE_DEFAULT_PERM_USER;
\c :TEST_DBNAME :ROLE_DEFAULT_PERM_USER
CREATE TABLE conditions(
    time TIMESTAMPTZ NOT NULL,
    device INTEGER,
    temperature FLOAT
);
SELECT * FROM create_hypertable('conditions', 'time',
                                chunk_time_interval => '1 day'::interval);
 hypertable_id | schema_name | table_name | created 
---------------+-------------+------------+---------
             7 | public      | conditions | t
(1 row)

INSERT INTO conditions
SELECT time, (random()*30)::int, random()*80 - 40
FROM generate_series('2018-12-01 00:00'::timestamp, '2018-12-31 00:00'::timestamp, '10 min') AS time;
CREATE MATERIALIZED VIEW conditions_summary
WITH (timescaledb.continuous) AS
SELECT device,
       time_bucket(INTERVAL '1 hour', "time") AS day,
       AVG(temperature) AS avg_temperature,
       MAX(temperature) AS max_temperature,
       MIN(temperature) AS min_temperature
FROM conditions
GROUP BY device, time_bucket(INTERVAL '1 hour', "time") WITH NO DATA;
CALL refresh_continuous_aggregate('conditions_summary', NULL, NULL);
ALTER TABLE conditions SET (timescaledb.compress);
SELECT COUNT(*) AS dropped_chunks_count
  FROM drop_chunks('conditions', TIMESTAMPTZ '2018-12-15 00:00');
 dropped_chunks_count 
----------------------
                   14
(1 row)

-- We need to have some chunks that are marked as dropped, otherwise
-- we will not have a problem below.
SELECT COUNT(*) AS dropped_chunks_count
  FROM _timescaledb_catalog.chunk
 WHERE dropped = TRUE;
 dropped_chunks_count 
----------------------
                   14
(1 row)

SELECT add_compression_policy AS job_id
  FROM add_compression_policy('conditions', INTERVAL '1 day') \gset
CALL run_job(:job_id);
\i include/recompress_basic.sql
-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
CREATE OR REPLACE VIEW compressed_chunk_info_view AS
SELECT
   h.schema_name AS hypertable_schema,
   h.table_name AS hypertable_name,
   c.schema_name as chunk_schema,
   c.table_name as chunk_name,
   c.status as chunk_status, 
   comp.schema_name as compressed_chunk_schema,
   comp.table_name as compressed_chunk_name
FROM
   _timescaledb_catalog.hypertable h JOIN
  _timescaledb_catalog.chunk c ON h.id = c.hypertable_id
   LEFT JOIN _timescaledb_catalog.chunk comp
ON comp.id = c.compressed_chunk_id
;
CREATE TABLE test2 (timec timestamptz NOT NULL, i integer ,
      b bigint, t text);
SELECT table_name from create_hypertable('test2', 'timec', chunk_time_interval=> INTERVAL '7 days');
 table_name 
------------
 test2
(1 row)

INSERT INTO test2 SELECT q, 10, 11, 'hello' FROM generate_series( '2020-01-03 10:00:00+00', '2020-01-03 12:00:00+00' , '5 min'::interval) q;
ALTER TABLE test2 set (timescaledb.compress, 
timescaledb.compress_segmentby = 'b', 
timescaledb.compress_orderby = 'timec DESC');
SELECT compress_chunk(c)
FROM show_chunks('test2') c;
              compress_chunk              
------------------------------------------
 _timescaledb_internal._hyper_10_48_chunk
(1 row)

---insert into the middle of the range ---
INSERT INTO test2 values ( '2020-01-03 10:01:00+00', 20, 11, '2row'); 
INSERT INTO test2 values ( '2020-01-03 11:01:00+00', 20, 11, '3row'); 
INSERT INTO test2 values ( '2020-01-03 12:01:00+00', 20, 11, '4row'); 
--- insert a new segment  by ---
INSERT INTO test2 values ( '2020-01-03 11:01:00+00', 20, 12, '12row'); 
SELECT time_bucket(INTERVAL '2 hour', timec), b, count(*)
FROM test2
GROUP BY time_bucket(INTERVAL '2 hour', timec), b
ORDER BY 1, 2;
         time_bucket          | b  | count 
------------------------------+----+-------
 Fri Jan 03 02:00:00 2020 PST | 11 |    26
 Fri Jan 03 02:00:00 2020 PST | 12 |     1
 Fri Jan 03 04:00:00 2020 PST | 11 |     2
(3 rows)

 
--check status for chunk --
SELECT chunk_status,
       chunk_name as "CHUNK_NAME"
FROM compressed_chunk_info_view 
WHERE hypertable_name = 'test2' ORDER BY chunk_name;
 chunk_status |     CHUNK_NAME     
--------------+--------------------
            3 | _hyper_10_48_chunk
(1 row)

SELECT compressed_chunk_schema || '.' || compressed_chunk_name as "COMP_CHUNK_NAME",
        chunk_schema || '.' || chunk_name as "CHUNK_NAME"
FROM compressed_chunk_info_view 
WHERE hypertable_name = 'test2' \gset
SELECT count(*) from test2;
 count 
-------
    29
(1 row)

SELECT recompress_chunk(:'CHUNK_NAME'::regclass); 
             recompress_chunk             
------------------------------------------
 _timescaledb_internal._hyper_10_48_chunk
(1 row)

SELECT chunk_status,
       chunk_name as "CHUNK_NAME"
FROM compressed_chunk_info_view 
WHERE hypertable_name = 'test2' ORDER BY chunk_name;
 chunk_status |     CHUNK_NAME     
--------------+--------------------
            1 | _hyper_10_48_chunk
(1 row)

--- insert into a compressed chunk again + a new chunk--
INSERT INTO test2 values ( '2020-01-03 11:01:03+00', 20, 11, '33row'), 
                         ( '2020-01-03 11:01:06+00', 20, 11, '36row'), 
                         ( '2020-01-03 11:02:00+00', 20, 12, '12row'), 
                         ( '2020-04-03 00:02:00+00', 30, 13, '3013row'); 
SELECT time_bucket(INTERVAL '2 hour', timec), b, count(*)
FROM test2
GROUP BY time_bucket(INTERVAL '2 hour', timec), b
ORDER BY 1, 2;
         time_bucket          | b  | count 
------------------------------+----+-------
 Fri Jan 03 02:00:00 2020 PST | 11 |    28
 Fri Jan 03 02:00:00 2020 PST | 12 |     2
 Fri Jan 03 04:00:00 2020 PST | 11 |     2
 Thu Apr 02 17:00:00 2020 PDT | 13 |     1
(4 rows)

--chunk status should be unordered for the previously compressed chunk
SELECT chunk_status,
       chunk_name as "CHUNK_NAME"
FROM compressed_chunk_info_view 
WHERE hypertable_name = 'test2' ORDER BY chunk_name;
 chunk_status |     CHUNK_NAME     
--------------+--------------------
            3 | _hyper_10_48_chunk
            0 | _hyper_10_51_chunk
(2 rows)

SELECT add_compression_policy AS job_id
  FROM add_compression_policy('test2', '30d'::interval) \gset
CALL run_job(:job_id);
CALL run_job(:job_id);
-- status should be compressed ---
SELECT chunk_status,
       chunk_name as "CHUNK_NAME"
FROM compressed_chunk_info_view 
WHERE hypertable_name = 'test2' ORDER BY chunk_name;
 chunk_status |     CHUNK_NAME     
--------------+--------------------
            1 | _hyper_10_48_chunk
            1 | _hyper_10_51_chunk
(2 rows)

\set ON_ERROR_STOP 0
-- call recompress_chunk when status is not unordered
SELECT recompress_chunk(:'CHUNK_NAME'::regclass, true); 
psql:include/recompress_basic.sql:96: NOTICE:  nothing to recompress in chunk "_hyper_10_48_chunk" 
 recompress_chunk 
------------------
 
(1 row)

SELECT recompress_chunk(:'CHUNK_NAME'::regclass, false); 
psql:include/recompress_basic.sql:97: ERROR:  nothing to recompress in chunk "_hyper_10_48_chunk" 
--now decompress it , then try and recompress 
SELECT decompress_chunk(:'CHUNK_NAME'::regclass);
             decompress_chunk             
------------------------------------------
 _timescaledb_internal._hyper_10_48_chunk
(1 row)

SELECT recompress_chunk(:'CHUNK_NAME'::regclass);
psql:include/recompress_basic.sql:101: ERROR:  call compress_chunk instead of recompress_chunk
\set ON_ERROR_STOP 1
-- test recompress policy
CREATE TABLE metrics(time timestamptz NOT NULL);
SELECT create_hypertable('metrics','time');
   create_hypertable   
-----------------------
 (12,public,metrics,t)
(1 row)

ALTER TABLE metrics SET (timescaledb.compress);
-- create chunk with some data and compress
INSERT INTO metrics SELECT '2000-01-01' FROM generate_series(1,10);
-- create custom compression job without recompress boolean
SELECT add_job('_timescaledb_internal.policy_compression','1w','{"hypertable_id": 12, "compress_after": "@ 7 days"}') AS "JOB_COMPRESS" \gset
-- first call should compress
CALL run_job(:JOB_COMPRESS);
-- 2nd call should do nothing
CALL run_job(:JOB_COMPRESS);
psql:include/recompress_basic.sql:118: NOTICE:  no chunks for hypertable public.metrics that satisfy compress chunk policy
---- status should be 1
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            1
(1 row)

-- do an INSERT so recompress has something to do
INSERT INTO metrics SELECT '2000-01-01';
---- status should be 3
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            3
(1 row)

-- should recompress
CALL run_job(:JOB_COMPRESS);
---- status should be 1
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            1
(1 row)

-- disable recompress in compress job
SELECT alter_job(id,config:=jsonb_set(config,'{recompress}','false')) FROM _timescaledb_config.bgw_job WHERE id = :JOB_COMPRESS;
                                                              alter_job                                                               
--------------------------------------------------------------------------------------------------------------------------------------
 (1004,"@ 7 days","@ 0",-1,"@ 5 mins",t,"{""recompress"": false, ""hypertable_id"": 12, ""compress_after"": ""@ 7 days""}",-infinity)
(1 row)

-- nothing to do
CALL run_job(:JOB_COMPRESS);
psql:include/recompress_basic.sql:139: NOTICE:  no chunks for hypertable public.metrics that satisfy compress chunk policy
---- status should be 1
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            1
(1 row)

-- do an INSERT so recompress has something to do
INSERT INTO metrics SELECT '2000-01-01';
---- status should be 3
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            3
(1 row)

-- still nothing to do since we disabled recompress
CALL run_job(:JOB_COMPRESS);
psql:include/recompress_basic.sql:151: NOTICE:  no chunks for hypertable public.metrics that satisfy compress chunk policy
---- status should be 3
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            3
(1 row)

-- reenable recompress in compress job
SELECT alter_job(id,config:=jsonb_set(config,'{recompress}','true')) FROM _timescaledb_config.bgw_job WHERE id = :JOB_COMPRESS;
                                                              alter_job                                                              
-------------------------------------------------------------------------------------------------------------------------------------
 (1004,"@ 7 days","@ 0",-1,"@ 5 mins",t,"{""recompress"": true, ""hypertable_id"": 12, ""compress_after"": ""@ 7 days""}",-infinity)
(1 row)

-- should recompress now
CALL run_job(:JOB_COMPRESS);
---- status should be 1
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            1
(1 row)

SELECT delete_job(:JOB_COMPRESS);
 delete_job 
------------
 
(1 row)

SELECT add_job('_timescaledb_internal.policy_recompression','1w','{"hypertable_id": 12, "recompress_after": "@ 7 days"}') AS "JOB_RECOMPRESS" \gset
---- status should be 1
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            1
(1 row)

---- nothing to do yet
CALL run_job(:JOB_RECOMPRESS);
psql:include/recompress_basic.sql:173: NOTICE:  no chunks for hypertable "public.metrics" that satisfy recompress chunk policy
---- status should be 1
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            1
(1 row)

-- create some work for recompress
INSERT INTO metrics SELECT '2000-01-01';
-- status should be 3
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            3
(1 row)

CALL run_job(:JOB_RECOMPRESS);
-- status should be 1
SELECT chunk_status FROM compressed_chunk_info_view WHERE hypertable_name = 'metrics';
 chunk_status 
--------------
            1
(1 row)

SELECT delete_job(:JOB_RECOMPRESS);
 delete_job 
------------
 
(1 row)

\i include/recompress_tuples.sql
-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
CREATE OR REPLACE VIEW compressed_chunk_info_view AS
SELECT
   h.schema_name AS hypertable_schema,
   h.table_name AS hypertable_name,
   c.schema_name as chunk_schema,
   c.table_name as chunk_name,
   c.status as chunk_status, 
   comp.schema_name as compressed_chunk_schema,
   comp.table_name as compressed_chunk_name
FROM
   _timescaledb_catalog.hypertable h JOIN
  _timescaledb_catalog.chunk c ON h.id = c.hypertable_id
   LEFT JOIN _timescaledb_catalog.chunk comp
ON comp.id = c.compressed_chunk_id
;
-- TEST1 test recompress_tuples has 1 output row per segment --
CREATE TABLE test3 (timec timestamptz NOT NULL, i integer ,
      segcol bigint, t text);
SELECT table_name from create_hypertable('test3', 'timec', chunk_time_interval=> INTERVAL '7 days');
 table_name 
------------
 test3
(1 row)

-- insert into 2 segments and then compress
INSERT INTO test3 SELECT q, 10, 11, 'hello' FROM generate_series( '2020-01-03 10:00:00+00', '2020-01-03 12:00:00+00' , '5 min'::interval) q;
INSERT INTO test3 SELECT q, 10, 15, 'hello' FROM generate_series( '2020-01-03 10:00:00+00', '2020-01-03 12:00:00+00' , '5 min'::interval) q;
ALTER TABLE test3 set (timescaledb.compress, 
timescaledb.compress_segmentby = 'segcol', 
timescaledb.compress_orderby = 'timec DESC');
SELECT compress_chunk(c)
FROM show_chunks('test3') c;
              compress_chunk              
------------------------------------------
 _timescaledb_internal._hyper_14_59_chunk
(1 row)

SELECT compressed_chunk_schema || '.' || compressed_chunk_name as "COMP_CHUNK_NAME",
        chunk_schema || '.' || chunk_name as "CHUNK_NAME"
FROM compressed_chunk_info_view 
WHERE hypertable_name = 'test3' \gset
--insert into middle of a range of compressed chunk
INSERT INTO test3 values ( '2020-01-03 10:01:00+00', 20, 11, '2row');
INSERT INTO test3 values ( '2020-01-03 11:01:00+00', 20, 11, '3row');
INSERT INTO test3 values ( '2020-01-03 12:01:00+00', 20, 11, '4row');
--- insert a new segment  by ---
INSERT INTO test3 values ( '2020-01-03 11:01:00+00', 20, 12, '12row');
SELECT segcol, _timescaledb_internal.recompress_tuples(:'CHUNK_NAME'::regclass, c ) arr 
FROM :COMP_CHUNK_NAME c  WHERE segcol = 11 group by segcol;
 segcol |                                                                                                                                                                                                         arr                                                                                                                                                                                                         
--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
     11 | {"(BAAAAj44JDWIAP/////8bHkAAAAAHAAAAAgAAAAA19197gAEfHOp4K4AAAR8c7EHu/8AAAAAHJw3/wAAAAAAAAAAFXUqAAcnDgAAAAAAHJw3/wAAAAAAAAAAFXUqAAcnDgA=,BAAAAAAAAAAACv/////////2AAAAHAAAAAMAAAAAAAAGZgAAAAAAAU7oAAAABSdQAAAAAJ1AAAAAAA==,11,AgBwZ19jYXRhbG9nAHRleHQAAAAAHAAAAAEAAAAAAAAAAgB1VVVZVVVUAAEAAAAEAAAABDRyb3cAAAAFaGVsbG8AAAAEM3JvdwAAAAQycm93,28,10,\"Fri Jan 03 02:00:00 2020 PST\",\"Fri Jan 03 04:01:00 2020 PST\")"}
(1 row)

\set TABLE_NAME test3 
\ir recompress_test.sql
-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
\set ECHO errors
 before_recompress_count 
-------------------------
                      54
(1 row)

 before_recompress_chunk_table_count 
-------------------------------------
                                   6
(1 row)

 timec | i | segcol | t 
-------+---+--------+---
(0 rows)

 timec | i | segcol | t 
-------+---+--------+---
(0 rows)

 after_recompress_count 
------------------------
                     54
(1 row)

 after_compress_chunk_table_count 
----------------------------------
                                3
(1 row)

 segcol | _ts_meta_count | _ts_meta_sequence_num |        _ts_meta_min_1        |        _ts_meta_max_1        
--------+----------------+-----------------------+------------------------------+------------------------------
     11 |             28 |                    10 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:01:00 2020 PST
     12 |              1 |                    10 | Fri Jan 03 03:01:00 2020 PST | Fri Jan 03 03:01:00 2020 PST
     15 |             25 |                    20 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:00:00 2020 PST
(3 rows)

 segcol | min 
--------+-----
     11 |  10
     12 |  10
     15 |  20
(3 rows)

 before_recompress_count 
-------------------------
                    3655
(1 row)

 before_recompress_chunk_table_count 
-------------------------------------
                                3604
(1 row)

 timec | i | segcol | t 
-------+---+--------+---
(0 rows)

 timec | i | segcol | t 
-------+---+--------+---
(0 rows)

 after_recompress_count 
------------------------
                   3655
(1 row)

 after_compress_chunk_table_count 
----------------------------------
                                6
(1 row)

 segcol | _ts_meta_count | _ts_meta_sequence_num |        _ts_meta_min_1        |        _ts_meta_max_1        
--------+----------------+-----------------------+------------------------------+------------------------------
     11 |             28 |                    10 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:01:00 2020 PST
     12 |           1000 |                    10 | Fri Jan 03 02:43:22 2020 PST | Fri Jan 03 03:01:00 2020 PST
     12 |           1000 |                    20 | Fri Jan 03 02:26:42 2020 PST | Fri Jan 03 02:43:21 2020 PST
     12 |           1000 |                    30 | Fri Jan 03 02:10:02 2020 PST | Fri Jan 03 02:26:41 2020 PST
     12 |            602 |                    40 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 02:10:01 2020 PST
     15 |             25 |                    20 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:00:00 2020 PST
(6 rows)

 segcol | min 
--------+-----
     11 |  10
     12 |  10
     15 |  20
(3 rows)

 before_recompress_count 
-------------------------
                   18178
(1 row)

 before_recompress_chunk_table_count 
-------------------------------------
                               14529
(1 row)

 timec | i | segcol | t 
-------+---+--------+---
(0 rows)

 timec | i | segcol | t 
-------+---+--------+---
(0 rows)

 after_recompress_count 
------------------------
                  18178
(1 row)

 after_compress_chunk_table_count 
----------------------------------
                               21
(1 row)

 segcol | _ts_meta_count | _ts_meta_sequence_num |        _ts_meta_min_1        |        _ts_meta_max_1        
--------+----------------+-----------------------+------------------------------+------------------------------
     11 |           1000 |                    10 | Fri Jan 03 04:43:21 2020 PST | Fri Jan 03 05:00:00 2020 PST
     11 |            829 |                    20 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:43:20 2020 PST
     12 |           1000 |                    10 | Fri Jan 03 04:13:21 2020 PST | Fri Jan 03 04:30:00 2020 PST
     12 |           1000 |                    20 | Fri Jan 03 03:56:41 2020 PST | Fri Jan 03 04:13:20 2020 PST
     12 |           1000 |                    30 | Fri Jan 03 03:40:01 2020 PST | Fri Jan 03 03:56:40 2020 PST
     12 |           1000 |                    40 | Fri Jan 03 03:23:21 2020 PST | Fri Jan 03 03:40:00 2020 PST
     12 |           1000 |                    50 | Fri Jan 03 03:06:41 2020 PST | Fri Jan 03 03:23:20 2020 PST
     12 |           1000 |                    60 | Fri Jan 03 02:51:03 2020 PST | Fri Jan 03 03:06:40 2020 PST
     12 |           1000 |                    70 | Fri Jan 03 02:34:23 2020 PST | Fri Jan 03 02:51:02 2020 PST
     12 |           1000 |                    80 | Fri Jan 03 02:17:43 2020 PST | Fri Jan 03 02:34:22 2020 PST
     12 |           1000 |                    90 | Fri Jan 03 02:01:03 2020 PST | Fri Jan 03 02:17:42 2020 PST
     12 |             63 |                   100 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 02:01:02 2020 PST
     14 |           1000 |                    10 | Fri Jan 03 04:43:21 2020 PST | Fri Jan 03 05:00:00 2020 PST
     14 |           1000 |                    20 | Fri Jan 03 04:26:41 2020 PST | Fri Jan 03 04:43:20 2020 PST
     14 |           1000 |                    30 | Fri Jan 03 04:10:01 2020 PST | Fri Jan 03 04:26:40 2020 PST
     14 |           1000 |                    40 | Fri Jan 03 03:53:21 2020 PST | Fri Jan 03 04:10:00 2020 PST
     14 |           1000 |                    50 | Fri Jan 03 03:36:41 2020 PST | Fri Jan 03 03:53:20 2020 PST
     14 |           1000 |                    60 | Fri Jan 03 03:20:01 2020 PST | Fri Jan 03 03:36:40 2020 PST
     14 |           1000 |                    70 | Fri Jan 03 03:03:21 2020 PST | Fri Jan 03 03:20:00 2020 PST
     14 |            261 |                    80 | Fri Jan 03 02:59:00 2020 PST | Fri Jan 03 03:03:20 2020 PST
     15 |             25 |                    20 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:00:00 2020 PST
(21 rows)

  table_name  
--------------
 test_multseg
(1 row)

              compress_chunk              
------------------------------------------
 _timescaledb_internal._hyper_16_61_chunk
(1 row)

 segcol | segcol2 | min 
--------+---------+-----
     11 |     100 |  10
     11 |     200 |  20
     15 |     900 |  30
(3 rows)

   compressed_chunk_name    |     chunk_name     | chunk_status 
----------------------------+--------------------+--------------
 compress_hyper_17_62_chunk | _hyper_16_61_chunk |            3
(1 row)

 before_recompress_count 
-------------------------
                      79
(1 row)

 before_recompress_chunk_table_count 
-------------------------------------
                                   7
(1 row)

         recompress_chunk_tuples          
------------------------------------------
 _timescaledb_internal._hyper_16_61_chunk
(1 row)

 timec | segcol2 | segcol | t 
-------+---------+--------+---
(0 rows)

 timec | segcol2 | segcol | t 
-------+---------+--------+---
(0 rows)

 after_recompress_count 
------------------------
                     79
(1 row)

 after_compress_chunk_table_count 
----------------------------------
                                4
(1 row)

 segcol | segcol2 | _ts_meta_count | _ts_meta_sequence_num |        _ts_meta_min_1        |        _ts_meta_max_1        
--------+---------+----------------+-----------------------+------------------------------+------------------------------
     11 |     100 |             26 |                    10 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:00:00 2020 PST
     11 |     200 |             26 |                    10 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:00:00 2020 PST
     12 |      20 |              1 |                    10 | Fri Jan 03 03:01:00 2020 PST | Fri Jan 03 03:01:00 2020 PST
     15 |     900 |             26 |                    10 | Fri Jan 03 02:00:00 2020 PST | Fri Jan 03 04:01:00 2020 PST
(4 rows)

   compressed_chunk_name    |     chunk_name     | chunk_status 
----------------------------+--------------------+--------------
 compress_hyper_17_62_chunk | _hyper_16_61_chunk |            1
(1 row)

